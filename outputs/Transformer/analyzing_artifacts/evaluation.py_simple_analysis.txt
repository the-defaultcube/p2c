<think>
Okay, I need to figure out how to write the logic analysis for evaluation.py based on the provided information. Let me start by understanding the requirements.

First, the evaluation.py file is supposed to handle two main tasks: calculating BLEU scores for translation and F1 scores for constituency parsing. It also needs to manage beam search for translation. 

Looking at the design document, the evaluation module should have functions for evaluation, BLEU calculation, and F1 scoring. The main.py will orchestrate these processes. 

I remember the paper mentioned using beam search with a beam size of 4 and a length penalty alpha of 0.6. Also, the maximum output length during inference is input length +50. These details should be included in the evaluation functions.

The configuration file specifies the beam size, alpha, and max output length, so I should reference these settings. However, the evaluation should also allow some flexibility, maybe using default values if the config isn't available.

For the BLEU score, I'll need a function that takes the model's predictions and the target texts. The function should compute the BLEU score using a library like sacrebleu. Similarly, for F1 score, I'll need to evaluate parsed trees against the gold standard, which might involve some tree matching algorithm.

I should also consider how to structure the evaluation function. It should probably take the model, data loader, and configuration, then run evaluation on the entire dataset, collecting metrics like BLEU or F1.

I need to make sure that the evaluation function returns a dictionary of metrics, which can then be used for reporting or comparisons. Also, handling cases where the configuration isn't provided by using default values is important for robustness.

Putting it all together, the evaluation.py should have clear functions for each metric, with proper documentation and parameters. It should integrate smoothly with the rest of the code, especially the trainer and main modules.

I should also think about any potential issues, like ensuring the correct tokenization during evaluation or handling cases where the model's output exceeds the max length. But for now, focusing on the core functionality as per the paper and plan is key.
</think>

```python
"""
evaluation.py
"""

import torch
from torch.utils.data import DataLoader
import numpy as np
from sacrebleu import BLEU
from sklearn.metrics import f1_score
from typing import Dict, Any
from dataset_loader import DatasetLoader

class Evaluator:
    def __init__(self, model: torch.nn.Module, eval_loader: DataLoader, config: Dict[str, Any] = None):
        """
        Initialize the evaluator with the model, data loader, and configuration.
        
        Args:
            model: The Transformer model to evaluate.
            eval_loader: DataLoader for the evaluation dataset.
            config: Configuration settings, including beam search parameters and maximum output length.
        """
        self.model = model
        self.eval_loader = eval_loader
        self.config = config if config else {}
        
        # Set default evaluation parameters if not provided in config
        self.beam_size = self.config.get('beam_size', 4)
        self.alpha = self.config.get('alpha', 0.6)
        self.max_output_length = self.config.get('max_output_length', 50)
        
        self.device = next(model.parameters()).device
    
    def evaluate(self) -> Dict[str, float]:
        """
        Perform evaluation on the model using the evaluation dataset.
        
        Returns:
            Dict containing evaluation metrics (BLEU score for translation, F1 score for parsing).
        """
        metrics = {'BLEU': 0.0, 'F1': 0.0}
        
        # Evaluate translation task
        bleu = self._compute_bleu()
        metrics['BLEU'] = bleu
        
        # Evaluate constituency parsing task
        f1 = self._compute_f1()
        metrics['F1'] = f1
        
        return metrics
    
    def _compute_bleu(self) -> float:
        """
        Compute the BLEU score for the translation task using beam search.
        
        Returns:
            BLEU score as a float.
        """
        bleu = BLEU(beam_size=self.beam_size, max_length=self.max_output_length, length_penalty=self.alpha)
        
        references = []
        hypotheses = []
        
        for batch in self.eval_loader:
            src, tgt = batch['src'], batch['tgt']
            src = src.to(self.device)
            
            # Generate translations using beam search
            with torch.no_grad():
                outputs = self.model.translate(src, self.beam_size, self.max_output_length, self.alpha)
            
            # Convert outputs and targets to strings
            for output, target in zip(outputs, tgt):
                hypotheses.append(output)
                references.append(target)
        
        # Calculate BLEU score
        bleu_score = bleu.corpus_score(hypotheses, [references])
        return bleu_score
    
    def _compute_f1(self) -> float:
        """
        Compute the F1 score for the constituency parsing task.
        
        Returns:
            F1 score as a float.
        """
        all_preds = []
        all_labels = []
        
        for batch in self.eval_loader:
            inputs, labels = batch['input'], batch['label']
            inputs = inputs.to(self.device)
            
            with torch.no_grad():
                outputs = self.model.parse_constituency(inputs)
            
            # Convert outputs and labels to numpy arrays
            all_preds.extend(outputs.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
        
        # Calculate F1 score
        f1 = f1_score(all_labels, all_preds, average='weighted')
        return f1
    
    @staticmethod
    def compute_bleu_score(hypotheses: List[str], references: List[str]) -> float:
        """
        Static method to compute BLEU score given hypotheses and references.
        
        Args:
            hypotheses: List of generated texts.
            references: List of reference texts.
            
        Returns:
            BLEU score as a float.
        """
        bleu = BLEU()
        return bleu.corpus_score(hypotheses, [references])
    
    @staticmethod
    def compute_f1_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """
        Static method to compute F1 score given true and predicted labels.
        
        Args:
            y_true: True labels as a numpy array.
            y_pred: Predicted labels as a numpy array.
            
        Returns:
            F1 score as a float.
        """
        return f1_score(y_true, y_pred, average='weighted')
```