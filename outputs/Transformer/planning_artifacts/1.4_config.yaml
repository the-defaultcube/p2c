## config.yaml
training:
  learning_rate: 0.0001
  batch_size: 25000
  epochs: 100000
  warmup_steps: 4000
  checkpoint_interval: 10  # minutes for base model
  evaluation_interval: 1000
  gradient_clip: 1.0

model:
  d_model: 512
  d_ff: 2048
  n_heads: 8
  num_layers: 6
  dropout: 0.1
  label_smoothing: 0.1

optimizer:
  type: Adam
  beta1: 0.9
  beta2: 0.98
  epsilon: 1e-9

dataset:
  name: WMT2014
  language_pair: [english_german, english_french]
  max_length: 100
  vocab_size: 32000

evaluation:
  beam_size: 4
  alpha: 0.6
  max_output_length: 50